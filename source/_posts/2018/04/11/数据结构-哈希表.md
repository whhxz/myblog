---
title: 数据结构-哈希表
date: 2018-04-11 11:18:57
categories: ['数据结构']
tags: ['哈希表', 'Hash一致']
---

哈希表（Hash Table，也叫散列表），是根据关键码值 (Key-Value) 而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。

在Java中HashMap就是使用的哈希表。

在HashMap中实际存储数据时在一个数组中，在插入时如果放入key，通过`(key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16)`获取hash，在数据存入table时，通过`(n - 1) & hash`获取数据应该存入数组下角位置。如果该位置存在数据，在JDK1.8之前是通过一个链表存入，如果重复就会吧数据放入该链表后，在JDK1.8里面是先通过链表存储，如果链表长度超过`TREEIFY_THRESHOLD`8通过红黑树来存储的数据。在取值时通过比较值来判断获取的key是否是传入当然key。
在转换树时，通过判断`MIN_TREEIFY_CAPACITY`64，超过才会转换为树，为了避免在哈希表建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。

在插入数据时，如果数据满了，当然不可能每次都放入链表或树中，这样在数据量较多的时候，会严重影响效率。在HashMap中有个扩容因子`DEFAULT_LOAD_FACTOR`，当插入数据后，数据大于该扩容因子，那么会把数据进行2倍的扩容，在扩容时，如果红黑树的长度小于`UNTREEIFY_THRESHOLD`6则会退化采用链表，同时把原有的数据重新插入。


理想状态下哈希表的节点中，元素的数量遵守泊松分布。当负载因子为 0.75 时，泊松公式中 λ 约等于 0.5，因此箱子中元素个数和概率的关系如下：

| 数量 | 概率 | 
| :--: |:-----:|
| 0 | 0.60653066 | 
| 1 | 0.30326533 | 
| 2 | 0.07581633 | 
| 3 | 0.01263606 | 
| 4 | 0.00157952 | 
| 5 | 0.00015795 | 
| 6 | 0.00001316 | 
| 7 | 0.00000094 | 
| 8 | 0.00000006 |

当数量为8时，概率极低，如果出现数量为8的情况，那么可能是因为hash函数设计导致问题，所以需要避免因为哈希函数导致性能问题，对链表转换为红黑树。

在插入数据时，获取key的hash需要满足散列结果应当具有同一性（输出值尽量均匀）和雪崩效应（微小的输入值变化使得输出值发生巨大的变化）。这样就能减少出现hash碰撞的情况。

通过python实现简单版hashmap
```py
class HashMap:
    def __init__(self):
        # 存储数据数组
        self.table = [None] * 10
        self.size = 0
        # 触发扩容临界点
        self.threshold = 1.5

    # map放入数据
    def put(self, key, val):
        add_node = self.__put_val(self.table, key, val)
        # 如果是新增节点长度增加，且检查扩容
        if add_node:
            self.size += 1
            if self.size * self.threshold >= self.threshold * len(self.table):
                self.resize()

    def __put_val(self, table, key, val):
        add_node = False
        # 获取key对呀节点值
        key_hash = self.key_hash(key)
        node = table[key_hash]
        # 不存在直接插入数组
        if node is None:
            table[key_hash] = LinkedNode(key_hash, key, val)
            add_node = True
        else:
            # 存在遍历链接
            while True:
                # key相同替换
                if node.key == key:
                    node.val = val
                    break
                # 不存在key，加入末尾节点
                if node.next is None:
                    node.next = LinkedNode(key_hash, key, val)
                    add_node = True
                    break
                else:
                    node = node.next
        return add_node

    # key获取值
    def get(self, key):
        key_hash = self.key_hash(key)
        node = self.table[key_hash]
        val = None
        # 遍历节点
        while True:
            if node is not None:
                # 比较节点key
                if node.key == key:
                    val = node.val
                    break
                else:
                    node = node.next
            else:
                break

        return val

    # 计算key哈希，只是简单存入数值，直接对数值长度取模
    def key_hash(self, key):
        return key % len(self.table)

    # 扩容，直接创建新数组，迁移旧数组到新数组
    def resize(self):
        new_table = [None] * len(self.table) * 2
        for node in self.table:
            while True:
                if node is not None:
                    self.__put_val(new_table, node.key, node.val)
                    node = node.next
                else:
                    break

        self.table = new_table


# map节点中存入的节点
class LinkedNode:
    def __init__(self, hash, key, val):
        self.hash = hash
        self.key = key
        self.val = val
        self.next = None
```
上述只是简单实现，在计算hash时直接通过取模来获取。

上述使用hashmap有个缺点在于，如果数据量非常大，那么每次扩容迁移数据都得花费不少时间，因为要把旧数据迁移到新的数组中。

### redis hash扩容
在上述扩容过程中比较慢，特别是数据量大时，但是在redis中hash扩容采用了另外一种机制。

在redis中时间存储简单理解为：redis中存储有两个数组，数组ht[0]，存储原本的数据，数组ht[1]用于扩容，同时有个字段`rehashidx`用来标识扩容进度。扩容如下：
1、先创建一个比ht[0]更大的table ht[1]
2、将ht[0]中的数据一步步迁移到ht[1]中
3、迁移完毕后，将ht[0]中数据清除，释放内存，将ht[1]替换为ht[0]

在扩容过程中，并不是一次性完成。避免因为服务器阻塞导致性能下降
1、_dictRehashStep被动迁移，一般是在插入、查找、删除都会触发执行，避免在一次操作中执行
2、dictRehashMilliseconds，服务器常规任务程序（server cron job）执行

在迁移过程中，查找、删除会在ht[0]、ht[1]中进行，新增只会在ht[1]中进行，这样保证了ht[0]只减不增。

除了扩容，还是缩容，操作与上述类型，创建一个小的table然后进行迁移。

* 在redis扩容过程中，链表迁移时，会把数据放到链表前面，这样的好处是，插入较快，而且新插入的数据可能会频繁的获取。

### 动态hash


参考：
* [深入理解哈希表](https://bestswifter.com/hashtable/)
* [字典](http://origin.redisbook.com/internal-datastruct/dict.html)